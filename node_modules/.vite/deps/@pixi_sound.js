import {
  BaseTexture,
  ExtensionType,
  LoaderParserPriority,
  Ticker,
  extensions,
  lib_exports,
  settings
} from "./chunk-DSWJHRJ6.js";
import {
  __export
} from "./chunk-5WWUZCGV.js";

// node_modules/@pixi/sound/lib/instance.mjs
var instance;
function setInstance(sound2) {
  instance = sound2;
  return sound2;
}
function getInstance() {
  return instance;
}

// node_modules/@pixi/sound/lib/htmlaudio/HTMLAudioInstance.mjs
var id = 0;
var _HTMLAudioInstance = class extends lib_exports.EventEmitter {
  /** @param parent - Parent element */
  constructor(parent) {
    super();
    this.id = id++;
    this.init(parent);
  }
  /**
   * Set a property by name, this makes it easy to chain values
   * @param name - Name of the property to set
   * @param value - Value to set property to
   */
  set(name, value) {
    if (this[name] === void 0) {
      throw new Error(`Property with name ${name} does not exist.`);
    } else {
      switch (name) {
        case "speed":
          this.speed = value;
          break;
        case "volume":
          this.volume = value;
          break;
        case "paused":
          this.paused = value;
          break;
        case "loop":
          this.loop = value;
          break;
        case "muted":
          this.muted = value;
          break;
      }
    }
    return this;
  }
  /** The current playback progress from 0 to 1. */
  get progress() {
    const { currentTime } = this._source;
    return currentTime / this._duration;
  }
  /** Pauses the sound. */
  get paused() {
    return this._paused;
  }
  set paused(paused) {
    this._paused = paused;
    this.refreshPaused();
  }
  /**
   * Reference: http://stackoverflow.com/a/40370077
   * @private
   */
  _onPlay() {
    this._playing = true;
  }
  /**
   * Reference: http://stackoverflow.com/a/40370077
   * @private
   */
  _onPause() {
    this._playing = false;
  }
  /**
   * Initialize the instance.
   * @param {htmlaudio.HTMLAudioMedia} media - Same as constructor
   */
  init(media) {
    this._playing = false;
    this._duration = media.source.duration;
    const source = this._source = media.source.cloneNode(false);
    source.src = media.parent.url;
    source.onplay = this._onPlay.bind(this);
    source.onpause = this._onPause.bind(this);
    media.context.on("refresh", this.refresh, this);
    media.context.on("refreshPaused", this.refreshPaused, this);
    this._media = media;
  }
  /**
   * Stop the sound playing
   * @private
   */
  _internalStop() {
    if (this._source && this._playing) {
      this._source.onended = null;
      this._source.pause();
    }
  }
  /** Stop the sound playing */
  stop() {
    this._internalStop();
    if (this._source) {
      this.emit("stop");
    }
  }
  /** Set the instance speed from 0 to 1 */
  get speed() {
    return this._speed;
  }
  set speed(speed) {
    this._speed = speed;
    this.refresh();
  }
  /** Get the set the volume for this instance from 0 to 1 */
  get volume() {
    return this._volume;
  }
  set volume(volume) {
    this._volume = volume;
    this.refresh();
  }
  /** If the sound instance should loop playback */
  get loop() {
    return this._loop;
  }
  set loop(loop) {
    this._loop = loop;
    this.refresh();
  }
  /** `true` if the sound is muted */
  get muted() {
    return this._muted;
  }
  set muted(muted) {
    this._muted = muted;
    this.refresh();
  }
  /**
   * HTML Audio does not support filters, this is non-functional API.
   */
  get filters() {
    console.warn("HTML Audio does not support filters");
    return null;
  }
  set filters(_filters) {
    console.warn("HTML Audio does not support filters");
  }
  /** Call whenever the loop, speed or volume changes */
  refresh() {
    const global = this._media.context;
    const sound2 = this._media.parent;
    this._source.loop = this._loop || sound2.loop;
    const globalVolume = global.volume * (global.muted ? 0 : 1);
    const soundVolume = sound2.volume * (sound2.muted ? 0 : 1);
    const instanceVolume = this._volume * (this._muted ? 0 : 1);
    this._source.volume = instanceVolume * globalVolume * soundVolume;
    this._source.playbackRate = this._speed * global.speed * sound2.speed;
  }
  /** Handle changes in paused state, either globally or sound or instance */
  refreshPaused() {
    const global = this._media.context;
    const sound2 = this._media.parent;
    const pausedReal = this._paused || sound2.paused || global.paused;
    if (pausedReal !== this._pausedReal) {
      this._pausedReal = pausedReal;
      if (pausedReal) {
        this._internalStop();
        this.emit("paused");
      } else {
        this.emit("resumed");
        this.play({
          start: this._source.currentTime,
          end: this._end,
          volume: this._volume,
          speed: this._speed,
          loop: this._loop
        });
      }
      this.emit("pause", pausedReal);
    }
  }
  /** Start playing the sound/ */
  play(options) {
    const { start, end, speed, loop, volume, muted } = options;
    if (end) {
      console.assert(end > start, "End time is before start time");
    }
    this._speed = speed;
    this._volume = volume;
    this._loop = !!loop;
    this._muted = muted;
    this.refresh();
    if (this.loop && end !== null) {
      console.warn('Looping not support when specifying an "end" time');
      this.loop = false;
    }
    this._start = start;
    this._end = end || this._duration;
    this._start = Math.max(0, this._start - _HTMLAudioInstance.PADDING);
    this._end = Math.min(this._end + _HTMLAudioInstance.PADDING, this._duration);
    this._source.onloadedmetadata = () => {
      if (this._source) {
        this._source.currentTime = start;
        this._source.onloadedmetadata = null;
        this.emit("progress", start, this._duration);
        Ticker.shared.add(this._onUpdate, this);
      }
    };
    this._source.onended = this._onComplete.bind(this);
    this._source.play();
    this.emit("start");
  }
  /**
   * Handle time update on sound.
   * @private
   */
  _onUpdate() {
    this.emit("progress", this.progress, this._duration);
    if (this._source.currentTime >= this._end && !this._source.loop) {
      this._onComplete();
    }
  }
  /**
   * Callback when completed.
   * @private
   */
  _onComplete() {
    Ticker.shared.remove(this._onUpdate, this);
    this._internalStop();
    this.emit("progress", 1, this._duration);
    this.emit("end", this);
  }
  /** Don't use after this. */
  destroy() {
    Ticker.shared.remove(this._onUpdate, this);
    this.removeAllListeners();
    const source = this._source;
    if (source) {
      source.onended = null;
      source.onplay = null;
      source.onpause = null;
      this._internalStop();
    }
    this._source = null;
    this._speed = 1;
    this._volume = 1;
    this._loop = false;
    this._end = null;
    this._start = 0;
    this._duration = 0;
    this._playing = false;
    this._pausedReal = false;
    this._paused = false;
    this._muted = false;
    if (this._media) {
      this._media.context.off("refresh", this.refresh, this);
      this._media.context.off("refreshPaused", this.refreshPaused, this);
      this._media = null;
    }
  }
  /**
   * To string method for instance.
   * @return The string representation of instance.
   */
  toString() {
    return `[HTMLAudioInstance id=${this.id}]`;
  }
};
var HTMLAudioInstance = _HTMLAudioInstance;
HTMLAudioInstance.PADDING = 0.1;

// node_modules/@pixi/sound/lib/htmlaudio/HTMLAudioMedia.mjs
var HTMLAudioMedia = class extends lib_exports.EventEmitter {
  init(parent) {
    this.parent = parent;
    this._source = parent.options.source || new Audio();
    if (parent.url) {
      this._source.src = parent.url;
    }
  }
  // Implement create
  create() {
    return new HTMLAudioInstance(this);
  }
  /**
   * If the audio media is playable (ready).
   * @readonly
   */
  get isPlayable() {
    return !!this._source && this._source.readyState === 4;
  }
  /**
   * THe duration of the media in seconds.
   * @readonly
   */
  get duration() {
    return this._source.duration;
  }
  /**
   * Reference to the context.
   * @readonly
   */
  get context() {
    return this.parent.context;
  }
  /** The collection of filters, does not apply to HTML Audio. */
  get filters() {
    return null;
  }
  set filters(_filters) {
    console.warn("HTML Audio does not support filters");
  }
  // Override the destroy
  destroy() {
    this.removeAllListeners();
    this.parent = null;
    if (this._source) {
      this._source.src = "";
      this._source.load();
      this._source = null;
    }
  }
  /**
   * Get the audio source element.
   * @type {HTMLAudioElement}
   * @readonly
   */
  get source() {
    return this._source;
  }
  // Implement the method to being preloading
  load(callback) {
    const source = this._source;
    const sound2 = this.parent;
    if (source.readyState === 4) {
      sound2.isLoaded = true;
      const instance2 = sound2.autoPlayStart();
      if (callback) {
        setTimeout(() => {
          callback(null, sound2, instance2);
        }, 0);
      }
      return;
    }
    if (!sound2.url) {
      callback(new Error("sound.url or sound.source must be set"));
      return;
    }
    source.src = sound2.url;
    const onLoad = () => {
      removeListeners();
      sound2.isLoaded = true;
      const instance2 = sound2.autoPlayStart();
      if (callback) {
        callback(null, sound2, instance2);
      }
    };
    const onAbort = () => {
      removeListeners();
      if (callback) {
        callback(new Error("Sound loading has been aborted"));
      }
    };
    const onError = () => {
      removeListeners();
      const message = `Failed to load audio element (code: ${source.error.code})`;
      if (callback) {
        callback(new Error(message));
      } else {
        console.error(message);
      }
    };
    const removeListeners = () => {
      source.removeEventListener("canplaythrough", onLoad);
      source.removeEventListener("load", onLoad);
      source.removeEventListener("abort", onAbort);
      source.removeEventListener("error", onError);
    };
    source.addEventListener("canplaythrough", onLoad, false);
    source.addEventListener("load", onLoad, false);
    source.addEventListener("abort", onAbort, false);
    source.addEventListener("error", onError, false);
    source.load();
  }
};

// node_modules/@pixi/sound/lib/SoundSprite.mjs
var SoundSprite = class {
  /**
   * @param parent - The parent sound
   * @param options - Data associated with object.
   */
  constructor(parent, options) {
    this.parent = parent;
    Object.assign(this, options);
    this.duration = this.end - this.start;
    console.assert(this.duration > 0, "End time must be after start time");
  }
  /**
   * Play the sound sprite.
   * @param {Function} [complete] - Function call when complete
   * @return Sound instance being played.
   */
  play(complete) {
    return this.parent.play({
      complete,
      speed: this.speed || this.parent.speed,
      end: this.end,
      start: this.start,
      loop: this.loop
    });
  }
  /** Destroy and don't use after this */
  destroy() {
    this.parent = null;
  }
};

// node_modules/@pixi/sound/lib/utils/supported.mjs
var extensions2 = [
  "ogg",
  "oga",
  "opus",
  "m4a",
  "mp3",
  "mpeg",
  "wav",
  "aiff",
  "wma",
  "mid",
  "caf"
];
var mimes = [
  "audio/mpeg",
  "audio/ogg"
];
var supported = {};
function validateFormats(typeOverrides) {
  const overrides = {
    m4a: "audio/mp4",
    oga: "audio/ogg",
    opus: 'audio/ogg; codecs="opus"',
    caf: 'audio/x-caf; codecs="opus"',
    ...typeOverrides || {}
  };
  const audio = document.createElement("audio");
  const formats = {};
  const no = /^no$/;
  extensions2.forEach((ext) => {
    const canByExt = audio.canPlayType(`audio/${ext}`).replace(no, "");
    const canByType = overrides[ext] ? audio.canPlayType(overrides[ext]).replace(no, "") : "";
    formats[ext] = !!canByExt || !!canByType;
  });
  Object.assign(supported, formats);
}
validateFormats();

// node_modules/@pixi/sound/lib/webaudio/WebAudioUtils.mjs
var WebAudioUtils = class {
  /**
   * Dezippering is removed in the future Web Audio API, instead
   * we use the `setValueAtTime` method, however, this is not available
   * in all environments (e.g., Android webview), so we fallback to the `value` setter.
   * @param param - AudioNode parameter object
   * @param value - Value to set
   * @return The value set
   */
  static setParamValue(param, value) {
    if (param.setValueAtTime) {
      const context = getInstance().context;
      param.setValueAtTime(value, context.audioContext.currentTime);
    } else {
      param.value = value;
    }
    return value;
  }
};

// node_modules/@pixi/sound/lib/webaudio/WebAudioInstance.mjs
var id2 = 0;
var WebAudioInstance = class extends lib_exports.EventEmitter {
  constructor(media) {
    super();
    this.id = id2++;
    this._media = null;
    this._paused = false;
    this._muted = false;
    this._elapsed = 0;
    this.init(media);
  }
  /**
   * Set a property by name, this makes it easy to chain values
   * @param name - Name of the property to set.
   * @param value - Value to set property to.
   */
  set(name, value) {
    if (this[name] === void 0) {
      throw new Error(`Property with name ${name} does not exist.`);
    } else {
      switch (name) {
        case "speed":
          this.speed = value;
          break;
        case "volume":
          this.volume = value;
          break;
        case "muted":
          this.muted = value;
          break;
        case "loop":
          this.loop = value;
          break;
        case "paused":
          this.paused = value;
          break;
      }
    }
    return this;
  }
  /** Stops the instance, don't use after this. */
  stop() {
    if (this._source) {
      this._internalStop();
      this.emit("stop");
    }
  }
  /** Set the instance speed from 0 to 1 */
  get speed() {
    return this._speed;
  }
  set speed(speed) {
    this._speed = speed;
    this.refresh();
    this._update(true);
  }
  /** Get the set the volume for this instance from 0 to 1 */
  get volume() {
    return this._volume;
  }
  set volume(volume) {
    this._volume = volume;
    this.refresh();
  }
  /** `true` if the sound is muted */
  get muted() {
    return this._muted;
  }
  set muted(muted) {
    this._muted = muted;
    this.refresh();
  }
  /** If the sound instance should loop playback */
  get loop() {
    return this._loop;
  }
  set loop(loop) {
    this._loop = loop;
    this.refresh();
  }
  /** The collection of filters. */
  get filters() {
    return this._filters;
  }
  set filters(filters) {
    var _a;
    if (this._filters) {
      (_a = this._filters) == null ? void 0 : _a.filter((filter) => filter).forEach((filter) => filter.disconnect());
      this._filters = null;
      this._source.connect(this._gain);
    }
    this._filters = (filters == null ? void 0 : filters.length) ? filters.slice(0) : null;
    this.refresh();
  }
  /** Refresh loop, volume and speed based on changes to parent */
  refresh() {
    if (!this._source) {
      return;
    }
    const global = this._media.context;
    const sound2 = this._media.parent;
    this._source.loop = this._loop || sound2.loop;
    const globalVolume = global.volume * (global.muted ? 0 : 1);
    const soundVolume = sound2.volume * (sound2.muted ? 0 : 1);
    const instanceVolume = this._volume * (this._muted ? 0 : 1);
    WebAudioUtils.setParamValue(this._gain.gain, instanceVolume * soundVolume * globalVolume);
    WebAudioUtils.setParamValue(this._source.playbackRate, this._speed * sound2.speed * global.speed);
    this.applyFilters();
  }
  /** Connect filters nodes to audio context */
  applyFilters() {
    var _a;
    if ((_a = this._filters) == null ? void 0 : _a.length) {
      this._source.disconnect();
      let source = this._source;
      this._filters.forEach((filter) => {
        source.connect(filter.destination);
        source = filter;
      });
      source.connect(this._gain);
    }
  }
  /** Handle changes in paused state, either globally or sound or instance */
  refreshPaused() {
    const global = this._media.context;
    const sound2 = this._media.parent;
    const pausedReal = this._paused || sound2.paused || global.paused;
    if (pausedReal !== this._pausedReal) {
      this._pausedReal = pausedReal;
      if (pausedReal) {
        this._internalStop();
        this.emit("paused");
      } else {
        this.emit("resumed");
        this.play({
          start: this._elapsed % this._duration,
          end: this._end,
          speed: this._speed,
          loop: this._loop,
          volume: this._volume
        });
      }
      this.emit("pause", pausedReal);
    }
  }
  /**
   * Plays the sound.
   * @param options - Play options.
   */
  play(options) {
    const { start, end, speed, loop, volume, muted, filters } = options;
    if (end) {
      console.assert(end > start, "End time is before start time");
    }
    this._paused = false;
    const { source, gain } = this._media.nodes.cloneBufferSource();
    this._source = source;
    this._gain = gain;
    this._speed = speed;
    this._volume = volume;
    this._loop = !!loop;
    this._muted = muted;
    this._filters = filters;
    this.refresh();
    const duration = this._source.buffer.duration;
    this._duration = duration;
    this._end = end;
    this._lastUpdate = this._now();
    this._elapsed = start;
    this._source.onended = this._onComplete.bind(this);
    if (this._loop) {
      this._source.loopEnd = end;
      this._source.loopStart = start;
      this._source.start(0, start);
    } else if (end) {
      this._source.start(0, start, end - start);
    } else {
      this._source.start(0, start);
    }
    this.emit("start");
    this._update(true);
    this.enableTicker(true);
  }
  /** Start the update progress. */
  enableTicker(enabled) {
    Ticker.shared.remove(this._updateListener, this);
    if (enabled) {
      Ticker.shared.add(this._updateListener, this);
    }
  }
  /** The current playback progress from 0 to 1. */
  get progress() {
    return this._progress;
  }
  /** Pauses the sound. */
  get paused() {
    return this._paused;
  }
  set paused(paused) {
    this._paused = paused;
    this.refreshPaused();
  }
  /** Don't use after this. */
  destroy() {
    var _a;
    this.removeAllListeners();
    this._internalStop();
    if (this._gain) {
      this._gain.disconnect();
      this._gain = null;
    }
    if (this._media) {
      this._media.context.events.off("refresh", this.refresh, this);
      this._media.context.events.off("refreshPaused", this.refreshPaused, this);
      this._media = null;
    }
    (_a = this._filters) == null ? void 0 : _a.forEach((filter) => filter.disconnect());
    this._filters = null;
    this._end = null;
    this._speed = 1;
    this._volume = 1;
    this._loop = false;
    this._elapsed = 0;
    this._duration = 0;
    this._paused = false;
    this._muted = false;
    this._pausedReal = false;
  }
  /**
   * To string method for instance.
   * @return The string representation of instance.
   */
  toString() {
    return `[WebAudioInstance id=${this.id}]`;
  }
  /**
   * Get the current time in seconds.
   * @return Seconds since start of context
   */
  _now() {
    return this._media.context.audioContext.currentTime;
  }
  /** Callback for update listener */
  _updateListener() {
    this._update();
  }
  /** Internal update the progress. */
  _update(force = false) {
    if (this._source) {
      const now = this._now();
      const delta = now - this._lastUpdate;
      if (delta > 0 || force) {
        const speed = this._source.playbackRate.value;
        this._elapsed += delta * speed;
        this._lastUpdate = now;
        const duration = this._duration;
        let progress;
        if (this._source.loopStart) {
          const soundLength = this._source.loopEnd - this._source.loopStart;
          progress = (this._source.loopStart + this._elapsed % soundLength) / duration;
        } else {
          progress = this._elapsed % duration / duration;
        }
        this._progress = progress;
        this.emit("progress", this._progress, duration);
      }
    }
  }
  /** Initializes the instance. */
  init(media) {
    this._media = media;
    media.context.events.on("refresh", this.refresh, this);
    media.context.events.on("refreshPaused", this.refreshPaused, this);
  }
  /** Stops the instance. */
  _internalStop() {
    if (this._source) {
      this.enableTicker(false);
      this._source.onended = null;
      this._source.stop(0);
      this._source.disconnect();
      try {
        this._source.buffer = null;
      } catch (err) {
        console.warn("Failed to set AudioBufferSourceNode.buffer to null:", err);
      }
      this._source = null;
    }
  }
  /** Callback when completed. */
  _onComplete() {
    if (this._source) {
      this.enableTicker(false);
      this._source.onended = null;
      this._source.disconnect();
      try {
        this._source.buffer = null;
      } catch (err) {
        console.warn("Failed to set AudioBufferSourceNode.buffer to null:", err);
      }
    }
    this._source = null;
    this._progress = 1;
    this.emit("progress", 1, this._duration);
    this.emit("end", this);
  }
};

// node_modules/@pixi/sound/lib/Filterable.mjs
var Filterable = class {
  /**
   * @param input - The source audio node
   * @param output - The output audio node
   */
  constructor(input, output) {
    this._output = output;
    this._input = input;
  }
  /** The destination output audio node */
  get destination() {
    return this._input;
  }
  /** The collection of filters. */
  get filters() {
    return this._filters;
  }
  set filters(filters) {
    if (this._filters) {
      this._filters.forEach((filter) => {
        if (filter) {
          filter.disconnect();
        }
      });
      this._filters = null;
      this._input.connect(this._output);
    }
    if (filters && filters.length) {
      this._filters = filters.slice(0);
      this._input.disconnect();
      let prevFilter = null;
      filters.forEach((filter) => {
        if (prevFilter === null) {
          this._input.connect(filter.destination);
        } else {
          prevFilter.connect(filter.destination);
        }
        prevFilter = filter;
      });
      prevFilter.connect(this._output);
    }
  }
  /** Cleans up. */
  destroy() {
    this.filters = null;
    this._input = null;
    this._output = null;
  }
};

// node_modules/@pixi/sound/lib/webaudio/WebAudioNodes.mjs
var _WebAudioNodes = class extends Filterable {
  /**
   * @param context - The audio context.
   */
  constructor(context) {
    const audioContext = context.audioContext;
    const bufferSource = audioContext.createBufferSource();
    const gain = audioContext.createGain();
    const analyser = audioContext.createAnalyser();
    bufferSource.connect(analyser);
    analyser.connect(gain);
    gain.connect(context.destination);
    super(analyser, gain);
    this.context = context;
    this.bufferSource = bufferSource;
    this.gain = gain;
    this.analyser = analyser;
  }
  /**
   * Get the script processor node.
   * @readonly
   */
  get script() {
    if (!this._script) {
      this._script = this.context.audioContext.createScriptProcessor(_WebAudioNodes.BUFFER_SIZE);
      this._script.connect(this.context.destination);
    }
    return this._script;
  }
  /** Cleans up. */
  destroy() {
    super.destroy();
    this.bufferSource.disconnect();
    if (this._script) {
      this._script.disconnect();
    }
    this.gain.disconnect();
    this.analyser.disconnect();
    this.bufferSource = null;
    this._script = null;
    this.gain = null;
    this.analyser = null;
    this.context = null;
  }
  /**
   * Clones the bufferSource. Used just before playing a sound.
   * @returns {SourceClone} The clone AudioBufferSourceNode.
   */
  cloneBufferSource() {
    const orig = this.bufferSource;
    const source = this.context.audioContext.createBufferSource();
    source.buffer = orig.buffer;
    WebAudioUtils.setParamValue(source.playbackRate, orig.playbackRate.value);
    source.loop = orig.loop;
    const gain = this.context.audioContext.createGain();
    source.connect(gain);
    gain.connect(this.destination);
    return { source, gain };
  }
  /**
   * Get buffer size of `ScriptProcessorNode`.
   * @readonly
   */
  get bufferSize() {
    return this.script.bufferSize;
  }
};
var WebAudioNodes = _WebAudioNodes;
WebAudioNodes.BUFFER_SIZE = 0;

// node_modules/@pixi/sound/lib/webaudio/WebAudioMedia.mjs
var WebAudioMedia = class {
  /**
   * Re-initialize without constructing.
   * @param parent - - Instance of parent Sound container
   */
  init(parent) {
    this.parent = parent;
    this._nodes = new WebAudioNodes(this.context);
    this._source = this._nodes.bufferSource;
    this.source = parent.options.source;
  }
  /** Destructor, safer to use `SoundLibrary.remove(alias)` to remove this sound. */
  destroy() {
    this.parent = null;
    this._nodes.destroy();
    this._nodes = null;
    try {
      this._source.buffer = null;
    } catch (err) {
      console.warn("Failed to set AudioBufferSourceNode.buffer to null:", err);
    }
    this._source = null;
    this.source = null;
  }
  // Implement create
  create() {
    return new WebAudioInstance(this);
  }
  // Implement context
  get context() {
    return this.parent.context;
  }
  // Implement isPlayable
  get isPlayable() {
    return !!this._source && !!this._source.buffer;
  }
  // Implement filters
  get filters() {
    return this._nodes.filters;
  }
  set filters(filters) {
    this._nodes.filters = filters;
  }
  // Implements duration
  get duration() {
    console.assert(this.isPlayable, "Sound not yet playable, no duration");
    return this._source.buffer.duration;
  }
  /** Gets and sets the buffer. */
  get buffer() {
    return this._source.buffer;
  }
  set buffer(buffer) {
    this._source.buffer = buffer;
  }
  /** Get the current chained nodes object */
  get nodes() {
    return this._nodes;
  }
  // Implements load
  load(callback) {
    if (this.source) {
      this._decode(this.source, callback);
    } else if (this.parent.url) {
      this._loadUrl(callback);
    } else if (callback) {
      callback(new Error("sound.url or sound.source must be set"));
    } else {
      console.error("sound.url or sound.source must be set");
    }
  }
  /** Loads a sound using XHMLHttpRequest object. */
  async _loadUrl(callback) {
    const url = this.parent.url;
    const response = await settings.ADAPTER.fetch(url);
    this._decode(await response.arrayBuffer(), callback);
  }
  /**
   * Decodes the array buffer.
   * @param arrayBuffer - From load.
   * @param {Function} callback - Callback optional
   */
  _decode(arrayBuffer, callback) {
    const audioBufferReadyFn = (err, buffer) => {
      if (err) {
        if (callback) {
          callback(err);
        }
      } else {
        this.parent.isLoaded = true;
        this.buffer = buffer;
        const instance2 = this.parent.autoPlayStart();
        if (callback) {
          callback(null, this.parent, instance2);
        }
      }
    };
    if (arrayBuffer instanceof AudioBuffer) {
      audioBufferReadyFn(null, arrayBuffer);
    } else {
      const context = this.parent.context;
      context.decode(arrayBuffer, audioBufferReadyFn);
    }
  }
};

// node_modules/@pixi/sound/lib/Sound.mjs
var _Sound = class {
  /**
   * Create a new sound instance from source.
   * @param source - Either the path or url to the source file.
   *        or the object of options to use.
   * @return Created sound instance.
   */
  static from(source) {
    let options = {};
    if (typeof source === "string") {
      options.url = source;
    } else if (source instanceof ArrayBuffer || source instanceof AudioBuffer || source instanceof HTMLAudioElement) {
      options.source = source;
    } else if (Array.isArray(source)) {
      options.url = source;
    } else {
      options = source;
    }
    options = {
      autoPlay: false,
      singleInstance: false,
      url: null,
      source: null,
      preload: false,
      volume: 1,
      speed: 1,
      complete: null,
      loaded: null,
      loop: false,
      ...options
    };
    Object.freeze(options);
    const media = getInstance().useLegacy ? new HTMLAudioMedia() : new WebAudioMedia();
    return new _Sound(media, options);
  }
  /**
   * Use `Sound.from`
   * @ignore
   */
  constructor(media, options) {
    this.media = media;
    this.options = options;
    this._instances = [];
    this._sprites = {};
    this.media.init(this);
    const complete = options.complete;
    this._autoPlayOptions = complete ? { complete } : null;
    this.isLoaded = false;
    this._preloadQueue = null;
    this.isPlaying = false;
    this.autoPlay = options.autoPlay;
    this.singleInstance = options.singleInstance;
    this.preload = options.preload || this.autoPlay;
    this.url = Array.isArray(options.url) ? this.preferUrl(options.url) : options.url;
    this.speed = options.speed;
    this.volume = options.volume;
    this.loop = options.loop;
    if (options.sprites) {
      this.addSprites(options.sprites);
    }
    if (this.preload) {
      this._preload(options.loaded);
    }
  }
  /**
   * Internal help for resolving which file to use if there are multiple provide
   * this is especially helpful for working with bundlers (non Assets loading).
   */
  preferUrl(urls) {
    const [file] = urls.map((url) => ({ url, ext: lib_exports.path.extname(url).slice(1) })).filter(({ ext }) => supported[ext]).sort((a, b) => extensions2.indexOf(a.ext) - extensions2.indexOf(b.ext));
    if (!file) {
      throw new Error("No supported file type found");
    }
    return file.url;
  }
  /** Instance of the media context. */
  get context() {
    return getInstance().context;
  }
  /** Stops all the instances of this sound from playing. */
  pause() {
    this.isPlaying = false;
    this.paused = true;
    return this;
  }
  /** Resuming all the instances of this sound from playing */
  resume() {
    this.isPlaying = this._instances.length > 0;
    this.paused = false;
    return this;
  }
  /** Stops all the instances of this sound from playing. */
  get paused() {
    return this._paused;
  }
  set paused(paused) {
    this._paused = paused;
    this.refreshPaused();
  }
  /** The playback rate. */
  get speed() {
    return this._speed;
  }
  set speed(speed) {
    this._speed = speed;
    this.refresh();
  }
  /** Set the filters. Only supported with WebAudio. */
  get filters() {
    return this.media.filters;
  }
  set filters(filters) {
    this.media.filters = filters;
  }
  /**
   * @ignore
   */
  addSprites(source, data) {
    if (typeof source === "object") {
      const results = {};
      for (const alias in source) {
        results[alias] = this.addSprites(alias, source[alias]);
      }
      return results;
    }
    console.assert(!this._sprites[source], `Alias ${source} is already taken`);
    const sprite = new SoundSprite(this, data);
    this._sprites[source] = sprite;
    return sprite;
  }
  /** Destructor, safer to use `SoundLibrary.remove(alias)` to remove this sound. */
  destroy() {
    this._removeInstances();
    this.removeSprites();
    this.media.destroy();
    this.media = null;
    this._sprites = null;
    this._instances = null;
  }
  /**
   * Remove a sound sprite.
   * @param alias - The unique name of the sound sprite, if alias is omitted, removes all sprites.
   */
  removeSprites(alias) {
    if (!alias) {
      for (const name in this._sprites) {
        this.removeSprites(name);
      }
    } else {
      const sprite = this._sprites[alias];
      if (sprite !== void 0) {
        sprite.destroy();
        delete this._sprites[alias];
      }
    }
    return this;
  }
  /** If the current sound is playable (loaded). */
  get isPlayable() {
    return this.isLoaded && this.media && this.media.isPlayable;
  }
  /** Stops all the instances of this sound from playing. */
  stop() {
    if (!this.isPlayable) {
      this.autoPlay = false;
      this._autoPlayOptions = null;
      return this;
    }
    this.isPlaying = false;
    for (let i = this._instances.length - 1; i >= 0; i--) {
      this._instances[i].stop();
    }
    return this;
  }
  // Overloaded function
  play(source, complete) {
    let options;
    if (typeof source === "string") {
      const sprite = source;
      options = { sprite, loop: this.loop, complete };
    } else if (typeof source === "function") {
      options = {};
      options.complete = source;
    } else {
      options = source;
    }
    options = {
      complete: null,
      loaded: null,
      sprite: null,
      end: null,
      start: 0,
      volume: 1,
      speed: 1,
      muted: false,
      loop: false,
      ...options || {}
    };
    if (options.sprite) {
      const alias = options.sprite;
      console.assert(!!this._sprites[alias], `Alias ${alias} is not available`);
      const sprite = this._sprites[alias];
      options.start = sprite.start + (options.start || 0);
      options.end = sprite.end;
      options.speed = sprite.speed || 1;
      options.loop = sprite.loop || options.loop;
      delete options.sprite;
    }
    if (options.offset) {
      options.start = options.offset;
    }
    if (!this.isLoaded) {
      if (this._preloadQueue) {
        return new Promise((resolve) => {
          this._preloadQueue.push(() => {
            resolve(this.play(options));
          });
        });
      }
      this._preloadQueue = [];
      this.autoPlay = true;
      this._autoPlayOptions = options;
      return new Promise((resolve, reject) => {
        this._preload((err, sound2, media) => {
          this._preloadQueue.forEach((resolve2) => resolve2());
          this._preloadQueue = null;
          if (err) {
            reject(err);
          } else {
            if (options.loaded) {
              options.loaded(err, sound2, media);
            }
            resolve(media);
          }
        });
      });
    }
    if (this.singleInstance || options.singleInstance) {
      this._removeInstances();
    }
    const instance2 = this._createInstance();
    this._instances.push(instance2);
    this.isPlaying = true;
    instance2.once("end", () => {
      if (options.complete) {
        options.complete(this);
      }
      this._onComplete(instance2);
    });
    instance2.once("stop", () => {
      this._onComplete(instance2);
    });
    instance2.play(options);
    return instance2;
  }
  /** Internal only, speed, loop, volume change occured. */
  refresh() {
    const len = this._instances.length;
    for (let i = 0; i < len; i++) {
      this._instances[i].refresh();
    }
  }
  /** Handle changes in paused state. Internal only. */
  refreshPaused() {
    const len = this._instances.length;
    for (let i = 0; i < len; i++) {
      this._instances[i].refreshPaused();
    }
  }
  /** Gets and sets the volume. */
  get volume() {
    return this._volume;
  }
  set volume(volume) {
    this._volume = volume;
    this.refresh();
  }
  /** Gets and sets the muted flag. */
  get muted() {
    return this._muted;
  }
  set muted(muted) {
    this._muted = muted;
    this.refresh();
  }
  /** Gets and sets the looping. */
  get loop() {
    return this._loop;
  }
  set loop(loop) {
    this._loop = loop;
    this.refresh();
  }
  /** Starts the preloading of sound. */
  _preload(callback) {
    this.media.load(callback);
  }
  /** Gets the list of instances that are currently being played of this sound. */
  get instances() {
    return this._instances;
  }
  /** Get the map of sprites. */
  get sprites() {
    return this._sprites;
  }
  /** Get the duration of the audio in seconds. */
  get duration() {
    return this.media.duration;
  }
  /** Auto play the first instance. */
  autoPlayStart() {
    let instance2;
    if (this.autoPlay) {
      instance2 = this.play(this._autoPlayOptions);
    }
    return instance2;
  }
  /** Removes all instances. */
  _removeInstances() {
    for (let i = this._instances.length - 1; i >= 0; i--) {
      this._poolInstance(this._instances[i]);
    }
    this._instances.length = 0;
  }
  /**
   * Sound instance completed.
   * @param instance
   */
  _onComplete(instance2) {
    if (this._instances) {
      const index = this._instances.indexOf(instance2);
      if (index > -1) {
        this._instances.splice(index, 1);
      }
      this.isPlaying = this._instances.length > 0;
    }
    this._poolInstance(instance2);
  }
  /** Create a new instance. */
  _createInstance() {
    if (_Sound._pool.length > 0) {
      const instance2 = _Sound._pool.pop();
      instance2.init(this.media);
      return instance2;
    }
    return this.media.create();
  }
  /**
   * Destroy/recycling the instance object.
   * @param instance - Instance to recycle
   */
  _poolInstance(instance2) {
    instance2.destroy();
    if (_Sound._pool.indexOf(instance2) < 0) {
      _Sound._pool.push(instance2);
    }
  }
};
var Sound = _Sound;
Sound._pool = [];

// node_modules/@pixi/sound/lib/htmlaudio/HTMLAudioContext.mjs
var HTMLAudioContext = class extends lib_exports.EventEmitter {
  constructor() {
    super(...arguments);
    this.speed = 1;
    this.muted = false;
    this.volume = 1;
    this.paused = false;
  }
  /** Internal trigger when volume, mute or speed changes */
  refresh() {
    this.emit("refresh");
  }
  /** Internal trigger paused changes */
  refreshPaused() {
    this.emit("refreshPaused");
  }
  /**
   * HTML Audio does not support filters, this is non-functional API.
   */
  get filters() {
    console.warn("HTML Audio does not support filters");
    return null;
  }
  set filters(_filters) {
    console.warn("HTML Audio does not support filters");
  }
  /**
   * HTML Audio does not support `audioContext`
   * @readonly
   * @type {AudioContext}
   */
  get audioContext() {
    console.warn("HTML Audio does not support audioContext");
    return null;
  }
  /**
   * Toggles the muted state.
   * @return The current muted state.
   */
  toggleMute() {
    this.muted = !this.muted;
    this.refresh();
    return this.muted;
  }
  /**
   * Toggles the paused state.
   * @return The current paused state.
   */
  togglePause() {
    this.paused = !this.paused;
    this.refreshPaused();
    return this.paused;
  }
  /** Destroy and don't use after this */
  destroy() {
    this.removeAllListeners();
  }
};

// node_modules/@pixi/sound/lib/webaudio/WebAudioContext.mjs
var WebAudioContext = class _WebAudioContext extends Filterable {
  constructor() {
    const win = window;
    const ctx = new _WebAudioContext.AudioContext();
    const compressor = ctx.createDynamicsCompressor();
    const analyser = ctx.createAnalyser();
    analyser.connect(compressor);
    compressor.connect(ctx.destination);
    super(analyser, compressor);
    this.autoPause = true;
    this._ctx = ctx;
    this._offlineCtx = new _WebAudioContext.OfflineAudioContext(
      1,
      2,
      win.OfflineAudioContext ? Math.max(8e3, Math.min(96e3, ctx.sampleRate)) : 44100
    );
    this.compressor = compressor;
    this.analyser = analyser;
    this.events = new lib_exports.EventEmitter();
    this.volume = 1;
    this.speed = 1;
    this.muted = false;
    this.paused = false;
    this._locked = ctx.state === "suspended" && ("ontouchstart" in globalThis || "onclick" in globalThis);
    if (this._locked) {
      this._unlock();
      this._unlock = this._unlock.bind(this);
      document.addEventListener("mousedown", this._unlock, true);
      document.addEventListener("touchstart", this._unlock, true);
      document.addEventListener("touchend", this._unlock, true);
    }
    this.onFocus = this.onFocus.bind(this);
    this.onBlur = this.onBlur.bind(this);
    globalThis.addEventListener("focus", this.onFocus);
    globalThis.addEventListener("blur", this.onBlur);
  }
  /** Handle mobile WebAudio context resume */
  onFocus() {
    if (!this.autoPause) {
      return;
    }
    const state = this._ctx.state;
    if (state === "suspended" || state === "interrupted" || !this._locked) {
      this.paused = this._pausedOnBlur;
      this.refreshPaused();
    }
  }
  /** Handle mobile WebAudio context suspend */
  onBlur() {
    if (!this.autoPause) {
      return;
    }
    if (!this._locked) {
      this._pausedOnBlur = this._paused;
      this.paused = true;
      this.refreshPaused();
    }
  }
  /**
   * Try to unlock audio on iOS. This is triggered from either WebAudio plugin setup (which will work if inside of
   * a `mousedown` or `touchend` event stack), or the first document touchend/mousedown event. If it fails (touchend
   * will fail if the user presses for too long, indicating a scroll event instead of a click event.
   *
   * Note that earlier versions of iOS supported `touchstart` for this, but iOS9 removed this functionality. Adding
   * a `touchstart` event to support older platforms may preclude a `mousedown` even from getting fired on iOS9, so we
   * stick with `mousedown` and `touchend`.
   */
  _unlock() {
    if (!this._locked) {
      return;
    }
    this.playEmptySound();
    if (this._ctx.state === "running") {
      document.removeEventListener("mousedown", this._unlock, true);
      document.removeEventListener("touchend", this._unlock, true);
      document.removeEventListener("touchstart", this._unlock, true);
      this._locked = false;
    }
  }
  /**
   * Plays an empty sound in the web audio context.  This is used to enable web audio on iOS devices, as they
   * require the first sound to be played inside of a user initiated event (touch/click).
   */
  playEmptySound() {
    const source = this._ctx.createBufferSource();
    source.buffer = this._ctx.createBuffer(1, 1, 22050);
    source.connect(this._ctx.destination);
    source.start(0, 0, 0);
    if (source.context.state === "suspended") {
      source.context.resume();
    }
  }
  /**
   * Get AudioContext class, if not supported returns `null`
   * @type {AudioContext}
   * @readonly
   */
  static get AudioContext() {
    const win = window;
    return win.AudioContext || win.webkitAudioContext || null;
  }
  /**
   * Get OfflineAudioContext class, if not supported returns `null`
   * @type {OfflineAudioContext}
   * @readonly
   */
  static get OfflineAudioContext() {
    const win = window;
    return win.OfflineAudioContext || win.webkitOfflineAudioContext || null;
  }
  /** Destroy this context. */
  destroy() {
    super.destroy();
    const ctx = this._ctx;
    if (typeof ctx.close !== "undefined") {
      ctx.close();
    }
    globalThis.removeEventListener("focus", this.onFocus);
    globalThis.removeEventListener("blur", this.onBlur);
    this.events.removeAllListeners();
    this.analyser.disconnect();
    this.compressor.disconnect();
    this.analyser = null;
    this.compressor = null;
    this.events = null;
    this._offlineCtx = null;
    this._ctx = null;
  }
  /**
   * The WebAudio API AudioContext object.
   * @readonly
   * @type {AudioContext}
   */
  get audioContext() {
    return this._ctx;
  }
  /**
   * The WebAudio API OfflineAudioContext object.
   * @readonly
   * @type {OfflineAudioContext}
   */
  get offlineContext() {
    return this._offlineCtx;
  }
  /**
   * Pauses all sounds, even though we handle this at the instance
   * level, we'll also pause the audioContext so that the
   * time used to compute progress isn't messed up.
   * @default false
   */
  set paused(paused) {
    if (paused && this._ctx.state === "running") {
      this._ctx.suspend();
    } else if (!paused && this._ctx.state === "suspended") {
      this._ctx.resume();
    }
    this._paused = paused;
  }
  get paused() {
    return this._paused;
  }
  /** Emit event when muted, volume or speed changes */
  refresh() {
    this.events.emit("refresh");
  }
  /** Emit event when muted, volume or speed changes */
  refreshPaused() {
    this.events.emit("refreshPaused");
  }
  /**
   * Toggles the muted state.
   * @return The current muted state.
   */
  toggleMute() {
    this.muted = !this.muted;
    this.refresh();
    return this.muted;
  }
  /**
   * Toggles the paused state.
   * @return The current muted state.
   */
  togglePause() {
    this.paused = !this.paused;
    this.refreshPaused();
    return this._paused;
  }
  /**
   * Decode the audio data
   * @param arrayBuffer - Buffer from loader
   * @param callback - When completed, error and audioBuffer are parameters.
   */
  decode(arrayBuffer, callback) {
    const handleError = (err) => {
      callback(new Error((err == null ? void 0 : err.message) || "Unable to decode file"));
    };
    const result = this._offlineCtx.decodeAudioData(
      arrayBuffer,
      (buffer) => {
        callback(null, buffer);
      },
      handleError
    );
    if (result) {
      result.catch(handleError);
    }
  }
};

// node_modules/@pixi/sound/lib/SoundLibrary.mjs
var SoundLibrary = class {
  constructor() {
    this.init();
  }
  /**
   * Re-initialize the sound library, this will
   * recreate the AudioContext. If there's a hardware-failure
   * call `close` and then `init`.
   * @return Sound instance
   */
  init() {
    if (this.supported) {
      this._webAudioContext = new WebAudioContext();
    }
    this._htmlAudioContext = new HTMLAudioContext();
    this._sounds = {};
    this.useLegacy = !this.supported;
    return this;
  }
  /**
   * The global context to use.
   * @readonly
   */
  get context() {
    return this._context;
  }
  /**
   * Apply filters to all sounds. Can be useful
   * for setting global planning or global effects.
   * **Only supported with WebAudio.**
   * @example
   * import { sound, filters } from '@pixi/sound';
   * // Adds a filter to pan all output left
   * sound.filtersAll = [
   *     new filters.StereoFilter(-1)
   * ];
   */
  get filtersAll() {
    if (!this.useLegacy) {
      return this._context.filters;
    }
    return [];
  }
  set filtersAll(filtersAll) {
    if (!this.useLegacy) {
      this._context.filters = filtersAll;
    }
  }
  /**
   * `true` if WebAudio is supported on the current browser.
   */
  get supported() {
    return WebAudioContext.AudioContext !== null;
  }
  /**
   * @ignore
   */
  add(source, sourceOptions) {
    if (typeof source === "object") {
      const results = {};
      for (const alias in source) {
        const options2 = this._getOptions(
          source[alias],
          sourceOptions
        );
        results[alias] = this.add(alias, options2);
      }
      return results;
    }
    console.assert(!this._sounds[source], `Sound with alias ${source} already exists.`);
    if (sourceOptions instanceof Sound) {
      this._sounds[source] = sourceOptions;
      return sourceOptions;
    }
    const options = this._getOptions(sourceOptions);
    const sound2 = Sound.from(options);
    this._sounds[source] = sound2;
    return sound2;
  }
  /**
   * Internal methods for getting the options object
   * @private
   * @param source - The source options
   * @param overrides - Override default options
   * @return The construction options
   */
  _getOptions(source, overrides) {
    let options;
    if (typeof source === "string") {
      options = { url: source };
    } else if (Array.isArray(source)) {
      options = { url: source };
    } else if (source instanceof ArrayBuffer || source instanceof AudioBuffer || source instanceof HTMLAudioElement) {
      options = { source };
    } else {
      options = source;
    }
    options = { ...options, ...overrides || {} };
    return options;
  }
  /**
   * Do not use WebAudio, force the use of legacy. This **must** be called before loading any files.
   */
  get useLegacy() {
    return this._useLegacy;
  }
  set useLegacy(legacy) {
    this._useLegacy = legacy;
    this._context = !legacy && this.supported ? this._webAudioContext : this._htmlAudioContext;
  }
  /**
   * This disables auto-pause all playback when the window blurs (WebAudio only).
   * This is helpful to keep from playing sounds when the user switches tabs.
   * However, if you're running content within an iframe, this may be undesirable
   * and you should disable (set to `true`) this behavior.
   * @default false
   */
  get disableAutoPause() {
    return !this._webAudioContext.autoPause;
  }
  set disableAutoPause(autoPause) {
    this._webAudioContext.autoPause = !autoPause;
  }
  /**
   * Removes a sound by alias.
   * @param alias - The sound alias reference.
   * @return Instance for chaining.
   */
  remove(alias) {
    this.exists(alias, true);
    this._sounds[alias].destroy();
    delete this._sounds[alias];
    return this;
  }
  /**
   * Set the global volume for all sounds. To set per-sound volume see {@link SoundLibrary#volume}.
   */
  get volumeAll() {
    return this._context.volume;
  }
  set volumeAll(volume) {
    this._context.volume = volume;
    this._context.refresh();
  }
  /**
   * Set the global speed for all sounds. To set per-sound speed see {@link SoundLibrary#speed}.
   */
  get speedAll() {
    return this._context.speed;
  }
  set speedAll(speed) {
    this._context.speed = speed;
    this._context.refresh();
  }
  /**
   * Toggle paused property for all sounds.
   * @return `true` if all sounds are paused.
   */
  togglePauseAll() {
    return this._context.togglePause();
  }
  /**
   * Pauses any playing sounds.
   * @return Instance for chaining.
   */
  pauseAll() {
    this._context.paused = true;
    this._context.refreshPaused();
    return this;
  }
  /**
   * Resumes any sounds.
   * @return Instance for chaining.
   */
  resumeAll() {
    this._context.paused = false;
    this._context.refreshPaused();
    return this;
  }
  /**
   * Toggle muted property for all sounds.
   * @return `true` if all sounds are muted.
   */
  toggleMuteAll() {
    return this._context.toggleMute();
  }
  /**
   * Mutes all playing sounds.
   * @return Instance for chaining.
   */
  muteAll() {
    this._context.muted = true;
    this._context.refresh();
    return this;
  }
  /**
   * Unmutes all playing sounds.
   * @return Instance for chaining.
   */
  unmuteAll() {
    this._context.muted = false;
    this._context.refresh();
    return this;
  }
  /**
   * Stops and removes all sounds. They cannot be used after this.
   * @return Instance for chaining.
   */
  removeAll() {
    for (const alias in this._sounds) {
      this._sounds[alias].destroy();
      delete this._sounds[alias];
    }
    return this;
  }
  /**
   * Stops all sounds.
   * @return Instance for chaining.
   */
  stopAll() {
    for (const alias in this._sounds) {
      this._sounds[alias].stop();
    }
    return this;
  }
  /**
   * Checks if a sound by alias exists.
   * @param alias - Check for alias.
   * @param assert - Whether enable console.assert.
   * @return true if the sound exists.
   */
  exists(alias, assert = false) {
    const exists = !!this._sounds[alias];
    if (assert) {
      console.assert(exists, `No sound matching alias '${alias}'.`);
    }
    return exists;
  }
  /**
   * Convenience function to check to see if any sound is playing.
   * @returns `true` if any sound is currently playing.
   */
  isPlaying() {
    for (const alias in this._sounds) {
      if (this._sounds[alias].isPlaying) {
        return true;
      }
    }
    return false;
  }
  /**
   * Find a sound by alias.
   * @param alias - The sound alias reference.
   * @return Sound object.
   */
  find(alias) {
    this.exists(alias, true);
    return this._sounds[alias];
  }
  /**
   * Plays a sound.
   * @method play
   * @instance
   * @param {string} alias - The sound alias reference.
   * @param {string} sprite - The alias of the sprite to play.
   * @return {IMediaInstance|null} The sound instance, this cannot be reused
   *         after it is done playing. Returns `null` if the sound has not yet loaded.
   */
  /**
   * Plays a sound.
   * @param alias - The sound alias reference.
   * @param {PlayOptions|Function} options - The options or callback when done.
   * @return The sound instance,
   *        this cannot be reused after it is done playing. Returns a Promise if the sound
   *        has not yet loaded.
   */
  play(alias, options) {
    return this.find(alias).play(options);
  }
  /**
   * Stops a sound.
   * @param alias - The sound alias reference.
   * @return Sound object.
   */
  stop(alias) {
    return this.find(alias).stop();
  }
  /**
   * Pauses a sound.
   * @param alias - The sound alias reference.
   * @return Sound object.
   */
  pause(alias) {
    return this.find(alias).pause();
  }
  /**
   * Resumes a sound.
   * @param alias - The sound alias reference.
   * @return Instance for chaining.
   */
  resume(alias) {
    return this.find(alias).resume();
  }
  /**
   * Get or set the volume for a sound.
   * @param alias - The sound alias reference.
   * @param volume - Optional current volume to set.
   * @return The current volume.
   */
  volume(alias, volume) {
    const sound2 = this.find(alias);
    if (volume !== void 0) {
      sound2.volume = volume;
    }
    return sound2.volume;
  }
  /**
   * Get or set the speed for a sound.
   * @param alias - The sound alias reference.
   * @param speed - Optional current speed to set.
   * @return The current speed.
   */
  speed(alias, speed) {
    const sound2 = this.find(alias);
    if (speed !== void 0) {
      sound2.speed = speed;
    }
    return sound2.speed;
  }
  /**
   * Get the length of a sound in seconds.
   * @param alias - The sound alias reference.
   * @return The current duration in seconds.
   */
  duration(alias) {
    return this.find(alias).duration;
  }
  /**
   * Closes the sound library. This will release/destroy
   * the AudioContext(s). Can be used safely if you want to
   * initialize the sound library later. Use `init` method.
   */
  close() {
    this.removeAll();
    this._sounds = null;
    if (this._webAudioContext) {
      this._webAudioContext.destroy();
      this._webAudioContext = null;
    }
    if (this._htmlAudioContext) {
      this._htmlAudioContext.destroy();
      this._htmlAudioContext = null;
    }
    this._context = null;
    return this;
  }
};

// node_modules/@pixi/sound/lib/htmlaudio/index.mjs
var htmlaudio_exports = {};
__export(htmlaudio_exports, {
  HTMLAudioContext: () => HTMLAudioContext,
  HTMLAudioInstance: () => HTMLAudioInstance,
  HTMLAudioMedia: () => HTMLAudioMedia
});

// node_modules/@pixi/sound/lib/filters/index.mjs
var filters_exports = {};
__export(filters_exports, {
  DistortionFilter: () => DistortionFilter,
  EqualizerFilter: () => EqualizerFilter,
  Filter: () => Filter,
  MonoFilter: () => MonoFilter,
  ReverbFilter: () => ReverbFilter,
  StereoFilter: () => StereoFilter,
  StreamFilter: () => StreamFilter,
  TelephoneFilter: () => TelephoneFilter
});

// node_modules/@pixi/sound/lib/filters/Filter.mjs
var Filter = class {
  /**
   * @param {AudioNode} destination - The audio node to use as the destination for the input AudioNode
   * @param {AudioNode} [source] - Optional output node, defaults to destination node. This is useful
   *        when creating filters which contains multiple AudioNode elements chained together.
   */
  constructor(destination, source) {
    this.init(destination, source);
  }
  /** Reinitialize */
  init(destination, source) {
    this.destination = destination;
    this.source = source || destination;
  }
  /**
   * Connect to the destination.
   * @param {AudioNode} destination - The destination node to connect the output to
   */
  connect(destination) {
    var _a;
    (_a = this.source) == null ? void 0 : _a.connect(destination);
  }
  /** Completely disconnect filter from destination and source nodes. */
  disconnect() {
    var _a;
    (_a = this.source) == null ? void 0 : _a.disconnect();
  }
  /** Destroy the filter and don't use after this. */
  destroy() {
    this.disconnect();
    this.destination = null;
    this.source = null;
  }
};

// node_modules/@pixi/sound/lib/filters/EqualizerFilter.mjs
var _EqualizerFilter = class extends Filter {
  /**
   * @param f32 - Default gain for 32 Hz
   * @param f64 - Default gain for 64 Hz
   * @param f125 - Default gain for 125 Hz
   * @param f250 - Default gain for 250 Hz
   * @param f500 - Default gain for 500 Hz
   * @param f1k - Default gain for 1000 Hz
   * @param f2k - Default gain for 2000 Hz
   * @param f4k - Default gain for 4000 Hz
   * @param f8k - Default gain for 8000 Hz
   * @param f16k - Default gain for 16000 Hz
   */
  constructor(f32 = 0, f64 = 0, f125 = 0, f250 = 0, f500 = 0, f1k = 0, f2k = 0, f4k = 0, f8k = 0, f16k = 0) {
    let bands = [];
    const equalizerBands = [
      {
        f: _EqualizerFilter.F32,
        type: "lowshelf",
        gain: f32
      },
      {
        f: _EqualizerFilter.F64,
        type: "peaking",
        gain: f64
      },
      {
        f: _EqualizerFilter.F125,
        type: "peaking",
        gain: f125
      },
      {
        f: _EqualizerFilter.F250,
        type: "peaking",
        gain: f250
      },
      {
        f: _EqualizerFilter.F500,
        type: "peaking",
        gain: f500
      },
      {
        f: _EqualizerFilter.F1K,
        type: "peaking",
        gain: f1k
      },
      {
        f: _EqualizerFilter.F2K,
        type: "peaking",
        gain: f2k
      },
      {
        f: _EqualizerFilter.F4K,
        type: "peaking",
        gain: f4k
      },
      {
        f: _EqualizerFilter.F8K,
        type: "peaking",
        gain: f8k
      },
      {
        f: _EqualizerFilter.F16K,
        type: "highshelf",
        gain: f16k
      }
    ];
    if (!getInstance().useLegacy) {
      bands = equalizerBands.map((band) => {
        const node = getInstance().context.audioContext.createBiquadFilter();
        node.type = band.type;
        WebAudioUtils.setParamValue(node.Q, 1);
        node.frequency.value = band.f;
        WebAudioUtils.setParamValue(node.gain, band.gain);
        return node;
      });
    }
    super(bands[0], bands[bands.length - 1]);
    this.bands = bands;
    this.bandsMap = {};
    for (let i = 0; i < this.bands.length; i++) {
      const node = this.bands[i];
      if (i > 0) {
        this.bands[i - 1].connect(node);
      }
      this.bandsMap[node.frequency.value] = node;
    }
  }
  /**
   * Set gain on a specific frequency.
   * @param frequency - The frequency, see EqualizerFilter.F* for bands
   * @param gain - Recommended -40 to 40.
   */
  setGain(frequency, gain = 0) {
    if (!this.bandsMap[frequency]) {
      throw new Error(`No band found for frequency ${frequency}`);
    }
    WebAudioUtils.setParamValue(this.bandsMap[frequency].gain, gain);
  }
  /**
   * Get gain amount on a specific frequency.
   * @return The amount of gain set.
   */
  getGain(frequency) {
    if (!this.bandsMap[frequency]) {
      throw new Error(`No band found for frequency ${frequency}`);
    }
    return this.bandsMap[frequency].gain.value;
  }
  /**
   * Gain at 32 Hz frequencey.
   * @default 0
   */
  set f32(value) {
    this.setGain(_EqualizerFilter.F32, value);
  }
  get f32() {
    return this.getGain(_EqualizerFilter.F32);
  }
  /**
   * Gain at 64 Hz frequencey.
   * @default 0
   */
  set f64(value) {
    this.setGain(_EqualizerFilter.F64, value);
  }
  get f64() {
    return this.getGain(_EqualizerFilter.F64);
  }
  /**
   * Gain at 125 Hz frequencey.
   * @default 0
   */
  set f125(value) {
    this.setGain(_EqualizerFilter.F125, value);
  }
  get f125() {
    return this.getGain(_EqualizerFilter.F125);
  }
  /**
   * Gain at 250 Hz frequencey.
   * @default 0
   */
  set f250(value) {
    this.setGain(_EqualizerFilter.F250, value);
  }
  get f250() {
    return this.getGain(_EqualizerFilter.F250);
  }
  /**
   * Gain at 500 Hz frequencey.
   * @default 0
   */
  set f500(value) {
    this.setGain(_EqualizerFilter.F500, value);
  }
  get f500() {
    return this.getGain(_EqualizerFilter.F500);
  }
  /**
   * Gain at 1 KHz frequencey.
   * @default 0
   */
  set f1k(value) {
    this.setGain(_EqualizerFilter.F1K, value);
  }
  get f1k() {
    return this.getGain(_EqualizerFilter.F1K);
  }
  /**
   * Gain at 2 KHz frequencey.
   * @default 0
   */
  set f2k(value) {
    this.setGain(_EqualizerFilter.F2K, value);
  }
  get f2k() {
    return this.getGain(_EqualizerFilter.F2K);
  }
  /**
   * Gain at 4 KHz frequencey.
   * @default 0
   */
  set f4k(value) {
    this.setGain(_EqualizerFilter.F4K, value);
  }
  get f4k() {
    return this.getGain(_EqualizerFilter.F4K);
  }
  /**
   * Gain at 8 KHz frequencey.
   * @default 0
   */
  set f8k(value) {
    this.setGain(_EqualizerFilter.F8K, value);
  }
  get f8k() {
    return this.getGain(_EqualizerFilter.F8K);
  }
  /**
   * Gain at 16 KHz frequencey.
   * @default 0
   */
  set f16k(value) {
    this.setGain(_EqualizerFilter.F16K, value);
  }
  get f16k() {
    return this.getGain(_EqualizerFilter.F16K);
  }
  /** Reset all frequency bands to have gain of 0 */
  reset() {
    this.bands.forEach((band) => {
      WebAudioUtils.setParamValue(band.gain, 0);
    });
  }
  destroy() {
    this.bands.forEach((band) => {
      band.disconnect();
    });
    this.bands = null;
    this.bandsMap = null;
  }
};
var EqualizerFilter = _EqualizerFilter;
EqualizerFilter.F32 = 32;
EqualizerFilter.F64 = 64;
EqualizerFilter.F125 = 125;
EqualizerFilter.F250 = 250;
EqualizerFilter.F500 = 500;
EqualizerFilter.F1K = 1e3;
EqualizerFilter.F2K = 2e3;
EqualizerFilter.F4K = 4e3;
EqualizerFilter.F8K = 8e3;
EqualizerFilter.F16K = 16e3;

// node_modules/@pixi/sound/lib/filters/DistortionFilter.mjs
var DistortionFilter = class extends Filter {
  /** @param amount - The amount of distoration from 0 to 1. */
  constructor(amount = 0) {
    let distortion;
    if (!getInstance().useLegacy) {
      const { audioContext } = getInstance().context;
      distortion = audioContext.createWaveShaper();
    }
    super(distortion);
    this._distortion = distortion;
    this.amount = amount;
  }
  /** The amount of distortion to set. */
  set amount(value) {
    this._amount = value;
    if (getInstance().useLegacy) {
      return;
    }
    const scaledValue = value * 1e3;
    const samples = 44100;
    const curve = new Float32Array(samples);
    const deg = Math.PI / 180;
    let i = 0;
    let x;
    for (; i < samples; ++i) {
      x = i * 2 / samples - 1;
      curve[i] = (3 + scaledValue) * x * 20 * deg / (Math.PI + scaledValue * Math.abs(x));
    }
    this._distortion.curve = curve;
    this._distortion.oversample = "4x";
  }
  get amount() {
    return this._amount;
  }
  destroy() {
    this._distortion = null;
    super.destroy();
  }
};

// node_modules/@pixi/sound/lib/filters/StereoFilter.mjs
var StereoFilter = class extends Filter {
  /** @param pan - The amount of panning, -1 is left, 1 is right, 0 is centered. */
  constructor(pan = 0) {
    let stereo;
    let panner;
    let destination;
    if (!getInstance().useLegacy) {
      const { audioContext } = getInstance().context;
      if (audioContext.createStereoPanner) {
        stereo = audioContext.createStereoPanner();
        destination = stereo;
      } else {
        panner = audioContext.createPanner();
        panner.panningModel = "equalpower";
        destination = panner;
      }
    }
    super(destination);
    this._stereo = stereo;
    this._panner = panner;
    this.pan = pan;
  }
  /** Set the amount of panning, where -1 is left, 1 is right, and 0 is centered */
  set pan(value) {
    this._pan = value;
    if (this._stereo) {
      WebAudioUtils.setParamValue(this._stereo.pan, value);
    } else if (this._panner) {
      this._panner.setPosition(value, 0, 1 - Math.abs(value));
    }
  }
  get pan() {
    return this._pan;
  }
  destroy() {
    super.destroy();
    this._stereo = null;
    this._panner = null;
  }
};

// node_modules/@pixi/sound/lib/filters/ReverbFilter.mjs
var ReverbFilter = class extends Filter {
  /**
   * @param seconds - Seconds for reverb
   * @param decay - The decay length
   * @param reverse - Reverse reverb
   */
  constructor(seconds = 3, decay = 2, reverse = false) {
    super(null);
    this._seconds = this._clamp(seconds, 1, 50);
    this._decay = this._clamp(decay, 0, 100);
    this._reverse = reverse;
    this._rebuild();
  }
  /**
   * Clamp a value
   * @param value
   * @param min - Minimum value
   * @param max - Maximum value
   * @return Clamped number
   */
  _clamp(value, min, max) {
    return Math.min(max, Math.max(min, value));
  }
  /**
   * Length of reverb in seconds from 1 to 50
   * @default 3
   */
  get seconds() {
    return this._seconds;
  }
  set seconds(seconds) {
    this._seconds = this._clamp(seconds, 1, 50);
    this._rebuild();
  }
  /**
   * Decay value from 0 to 100
   * @default 2
   */
  get decay() {
    return this._decay;
  }
  set decay(decay) {
    this._decay = this._clamp(decay, 0, 100);
    this._rebuild();
  }
  /**
   * Reverse value from 0 to 1
   * @default false
   */
  get reverse() {
    return this._reverse;
  }
  set reverse(reverse) {
    this._reverse = reverse;
    this._rebuild();
  }
  /**
   * Utility function for building an impulse response
   * from the module parameters.
   */
  _rebuild() {
    if (getInstance().useLegacy) {
      return;
    }
    const { audioContext } = getInstance().context;
    const rate = audioContext.sampleRate;
    const length = rate * this._seconds;
    const impulse = audioContext.createBuffer(2, length, rate);
    const impulseL = impulse.getChannelData(0);
    const impulseR = impulse.getChannelData(1);
    let n;
    for (let i = 0; i < length; i++) {
      n = this._reverse ? length - i : i;
      impulseL[i] = (Math.random() * 2 - 1) * Math.pow(1 - n / length, this._decay);
      impulseR[i] = (Math.random() * 2 - 1) * Math.pow(1 - n / length, this._decay);
    }
    const convolver = audioContext.createConvolver();
    convolver.buffer = impulse;
    this.init(convolver);
  }
};

// node_modules/@pixi/sound/lib/filters/MonoFilter.mjs
var MonoFilter = class extends Filter {
  constructor() {
    let merger;
    let splitter;
    if (!getInstance().useLegacy) {
      const { audioContext } = getInstance().context;
      splitter = audioContext.createChannelSplitter();
      merger = audioContext.createChannelMerger();
      merger.connect(splitter);
    }
    super(merger, splitter);
    this._merger = merger;
  }
  destroy() {
    var _a;
    (_a = this._merger) == null ? void 0 : _a.disconnect();
    this._merger = null;
    super.destroy();
  }
};

// node_modules/@pixi/sound/lib/filters/StreamFilter.mjs
var StreamFilter = class extends Filter {
  constructor() {
    let destination;
    let source;
    if (!getInstance().useLegacy) {
      const { audioContext } = getInstance().context;
      destination = audioContext.createMediaStreamDestination();
      source = audioContext.createMediaStreamSource(destination.stream);
    }
    super(destination, source);
    this._stream = destination == null ? void 0 : destination.stream;
  }
  get stream() {
    return this._stream;
  }
  destroy() {
    this._stream = null;
    super.destroy();
  }
};

// node_modules/@pixi/sound/lib/filters/TelephoneFilter.mjs
var TelephoneFilter = class extends Filter {
  constructor() {
    let destination;
    let source;
    if (!getInstance().useLegacy) {
      const { audioContext } = getInstance().context;
      const lpf1 = audioContext.createBiquadFilter();
      const lpf2 = audioContext.createBiquadFilter();
      const hpf1 = audioContext.createBiquadFilter();
      const hpf2 = audioContext.createBiquadFilter();
      lpf1.type = "lowpass";
      WebAudioUtils.setParamValue(lpf1.frequency, 2e3);
      lpf2.type = "lowpass";
      WebAudioUtils.setParamValue(lpf2.frequency, 2e3);
      hpf1.type = "highpass";
      WebAudioUtils.setParamValue(hpf1.frequency, 500);
      hpf2.type = "highpass";
      WebAudioUtils.setParamValue(hpf2.frequency, 500);
      lpf1.connect(lpf2);
      lpf2.connect(hpf1);
      hpf1.connect(hpf2);
      destination = lpf1;
      source = hpf2;
    }
    super(destination, source);
  }
};

// node_modules/@pixi/sound/lib/webaudio/index.mjs
var webaudio_exports = {};
__export(webaudio_exports, {
  WebAudioContext: () => WebAudioContext,
  WebAudioInstance: () => WebAudioInstance,
  WebAudioMedia: () => WebAudioMedia,
  WebAudioNodes: () => WebAudioNodes,
  WebAudioUtils: () => WebAudioUtils
});

// node_modules/@pixi/sound/lib/utils/index.mjs
var utils_exports = {};
__export(utils_exports, {
  PLAY_ID: () => PLAY_ID,
  extensions: () => extensions2,
  mimes: () => mimes,
  playOnce: () => playOnce,
  render: () => render,
  sineTone: () => sineTone,
  supported: () => supported,
  validateFormats: () => validateFormats
});

// node_modules/@pixi/sound/lib/utils/playOnce.mjs
var PLAY_ID = 0;
function playOnce(url, callback) {
  const alias = `alias${PLAY_ID++}`;
  getInstance().add(alias, {
    url,
    preload: true,
    autoPlay: true,
    loaded: (err) => {
      if (err) {
        console.error(err);
        getInstance().remove(alias);
        if (callback) {
          callback(err);
        }
      }
    },
    complete: () => {
      getInstance().remove(alias);
      if (callback) {
        callback(null);
      }
    }
  });
  return alias;
}

// node_modules/@pixi/sound/lib/utils/render.mjs
function render(sound2, options) {
  const canvas = document.createElement("canvas");
  options = {
    width: 512,
    height: 128,
    fill: "black",
    ...options || {}
  };
  canvas.width = options.width;
  canvas.height = options.height;
  const baseTexture = BaseTexture.from(canvas);
  if (!(sound2.media instanceof WebAudioMedia)) {
    return baseTexture;
  }
  const media = sound2.media;
  console.assert(!!media.buffer, "No buffer found, load first");
  const context = canvas.getContext("2d");
  context.fillStyle = options.fill;
  const data = media.buffer.getChannelData(0);
  const step = Math.ceil(data.length / options.width);
  const amp = options.height / 2;
  for (let i = 0; i < options.width; i++) {
    let min = 1;
    let max = -1;
    for (let j = 0; j < step; j++) {
      const datum = data[i * step + j];
      if (datum < min) {
        min = datum;
      }
      if (datum > max) {
        max = datum;
      }
    }
    context.fillRect(i, (1 + min) * amp, 1, Math.max(1, (max - min) * amp));
  }
  return baseTexture;
}

// node_modules/@pixi/sound/lib/utils/sineTone.mjs
function sineTone(hertz = 200, seconds = 1) {
  const sound2 = Sound.from({
    singleInstance: true
  });
  if (!(sound2.media instanceof WebAudioMedia)) {
    return sound2;
  }
  const media = sound2.media;
  const context = sound2.context;
  const nChannels = 1;
  const sampleRate = 48e3;
  const amplitude = 2;
  const buffer = context.audioContext.createBuffer(
    nChannels,
    seconds * sampleRate,
    sampleRate
  );
  const fArray = buffer.getChannelData(0);
  for (let i = 0; i < fArray.length; i++) {
    const time = i / buffer.sampleRate;
    const angle = hertz * time * Math.PI;
    fArray[i] = Math.sin(angle) * amplitude;
  }
  media.buffer = buffer;
  sound2.isLoaded = true;
  return sound2;
}

// node_modules/@pixi/sound/lib/soundAsset.mjs
var getAlias = (asset) => {
  var _a;
  const url = asset.src;
  return ((_a = asset == null ? void 0 : asset.alias) == null ? void 0 : _a[0]) ?? lib_exports.path.basename(url, lib_exports.path.extname(url));
};
var soundAsset = {
  extension: ExtensionType.Asset,
  detection: {
    test: async () => true,
    add: async (formats) => [...formats, ...extensions2.filter((ext) => supported[ext])],
    remove: async (formats) => formats.filter((ext) => formats.includes(ext))
  },
  loader: {
    extension: {
      type: [ExtensionType.LoadParser],
      priority: LoaderParserPriority.High
    },
    /** Should we attempt to load this file? */
    test(url) {
      const ext = lib_exports.path.extname(url).slice(1);
      return !!supported[ext] || mimes.some((mime) => url.startsWith(`data:${mime}`));
    },
    /** Load the sound file, this is mostly handled by Sound.from() */
    async load(url, asset) {
      const sound2 = await new Promise((resolve, reject) => Sound.from({
        ...asset.data,
        url,
        preload: true,
        loaded(err, sound22) {
          var _a, _b;
          if (err) {
            reject(err);
          } else {
            resolve(sound22);
          }
          (_b = (_a = asset.data) == null ? void 0 : _a.loaded) == null ? void 0 : _b.call(_a, err, sound22);
        }
      }));
      getInstance().add(getAlias(asset), sound2);
      return sound2;
    },
    /** Remove the sound from the library */
    async unload(_sound, asset) {
      getInstance().remove(getAlias(asset));
    }
  }
};
extensions.add(soundAsset);

// node_modules/@pixi/sound/lib/index.mjs
var sound = setInstance(new SoundLibrary());
export {
  Filter,
  Filterable,
  Sound,
  SoundLibrary,
  SoundSprite,
  filters_exports as filters,
  htmlaudio_exports as htmlaudio,
  sound,
  soundAsset,
  utils_exports as utils,
  webaudio_exports as webaudio
};
//# sourceMappingURL=@pixi_sound.js.map
